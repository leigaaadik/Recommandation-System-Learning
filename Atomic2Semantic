# 推荐系统 ID 编码技术的演进：从 Atomic ID 到 Semantic ID

> 推荐系统的核心挑战之一是如何高效准确地表示海量的物品，物品表示经历了从“判别式推荐中的Atomic ID”到“生成式推荐中的Semantic ID”的重大范式转变。

## 一、传统深度学习推荐模型与Atomic ID

传统的深度学习推荐模型（DLRM）中，物品表示使用的是Atomic ID机制，每一个物品都有一个唯一对应的标识符（Token），例如快手平台上，一个美妆视频被标记为 `item_241922` 。模型通过维护一个巨大的Embedding Table，将这些高维稀疏的id映射为低维稠密向量。模型在大量的用户行为序列上学习，不断更新这些向量，从而得到能够在向量空间中反映用户偏好的向量表示，例如美妆视频`item_241922`的向量表示，应该和对美妆感兴趣的用户表示更加相近。

但是在工业场景中面对海量的数据时，Atomic ID存在三个致命缺陷：
- 巨大的词表和存储开销：模型必须为每一个id存储一个独立的embedding，假设快手平台上有1200w视频，Embedding维度为128，存储这些向量就需要1200w * 128的空间，随着物品数量的增加，Embedding的显存开销会显著增加，成为模型扩展的瓶颈。
- 冷启动问题：每一个id的embedding是随机初始化的，项目必须与用户产生交互，模型才能通过反向传播更新在向量空间中的位置。
- 语义割裂：每一个id本质上是正交的，项目之间的内容相似性无法被利用，假设`item_999999`是一个新上传的美妆视频，从内容上看与`item_241922`高度相似，但是对于模型来说，这只是两个毫无关联的索引，除非出现大量的共现行为——大量用户同时观看了这两个视频，否则模型无法学习到这两个模型的相似性。

## 二、生成式推荐与 Semantic ID (SID)
 
 Google在TIGER中提出了具有层级结构和内容语义的Semantic ID，不同于通过系统分配的毫无意义的Atomic ID（例如`item_241922`），SID是根据物品的内容特征（文本、图像等）生成的一串离散Token元组（例如 `(5, 23, 55)`）。通常内容相似的物品会共享相同的前缀，例如两个美妆类的视频，它们的SID都是5开头。有限的码本（Codebook）通过排列组合，能够表达大量的物品，例如3层码本，每层$256$个Token，仅需要存储$256 * 3 = 768$ 个向量，就可以表示 $256^3$ 个不同的物品。SID有效缓解了存储爆炸问题，对于冷启动样本，例如新上传的视频，只需根据内容生成SID，模型无需任何样本的交互数据即可开始推荐。


## 三、RQ-VAE算法流程

RQ-VAE（Residual-Quantized Variational AutoEncoder）用于生成具有语义层级结构的离散 Semantic ID。其核心思想是利用**残差量化**技术，将连续的语义向量由粗到细地拆解为多个离散编码。

> ![[Clipboard_Screenshot_1770291592.png]]

(1) **语义特征提取与降维 **，首先将物品的原始内容特征转化为适合量化的低维潜在向量：
*   **语义嵌入**：使用预训练编码器将物品的多模态内容特征（标题、描述、图片等）转化为高维语义嵌入向量 $\mathbf{x} \in \mathbb{R}^{768}$
*   **降维映射**：使用一个 DNN Encoder 将 $\mathbf{x}$ 映射为低维潜在向量 $\mathbf{z} \in \mathbb{R}^{32}$，作为量化过程的初始输入 $r_0$，DNN Encoder 由三层 MLP 组成（维度 512-256-128，激活函数 ReLU），最终输出维度

(2) **迭代残差量化**，将低维向量 $\mathbf{z}$ 分解为 $M$ 个离散编码（论文中 $M=3$，每个码本大小 $K=256$）。为了防止码本坍塌，训练开始前使用 **K-Means** 对码本向量进行初始化。

*   **迭代计算**（对于第 $d$ 层， $d \in \{0, 1, 2\}$）：
    1.  **最近邻匹配**：在第 $d$ 层码本 $\mathcal{C}_d$ 中，寻找与当前残差 $r_d$ 欧氏距离最近的码本向量 $\mathbf{e}_{k}$：
        $$k^* = \arg\min_{k} \| r_d - \mathbf{e}_{k} \|_2$$
    2.  **生成 Token**：记录索引 $c_d = k^*$，作为 SID 在该层的值。
    3.  **计算新残差**：将当前残差减去选中的码本向量，得到下一层的输入（捕捉未被当前层表达的细粒度信息）：
        $$r_{d+1} = r_d - \mathbf{e}_{c_d}$$
    4.  **近似重构**：该层对原始向量的贡献为 $\mathbf{e}_{c_d}$。

(3) **最终表示与碰撞处理**
*   **Semantic ID**：基础形式为 $(c_0, c_1, c_2)$。为了解决不同物品映射到相同 ID 的**碰撞问题**，TIGER 会追加**第 4 位 Token**（如 0, 1...）以确保每个物品 ID 的唯一性。
*   **量化后的潜在向量**：$\hat{\mathbf{z}} = \sum_{d=0}^{M-1} \mathbf{e}_{c_d}$。

(4) ** 解码与重构**
*   将 $\hat{\mathbf{z}}$ 输入到一个与 Encoder 架构一致的 DNN Decoder，输出重构向量 $\hat{\mathbf{x}} \in \mathbb{R}^{768}$。

(5) **损失函数**，包含两部分：
*   **重构损失 ($L_{\text{recon}}$)**，衡量量化特征还原原始语义的能力：
    $$L_{\text{recon}} = \|\mathbf{x} - \hat{\mathbf{x}}\|^2$$
*   **RQ-VAE 损失 ($L_{\text{rqvae}}$)**用于更新码本并稳定训练。其中 `sg` (Stop Gradient) 表示切断梯度，损失函数中第一项更新码本，用于拉近码本向量 $\mathbf{e}_{c_d}$ 向输入残差 $r_d$ 靠拢，第二项更新Encoder，约束 Encoder 输出的残差 $r_d$ 不偏离选中的码本向量，防止训练发散。
    $$L_{\text{rqvae}} = \sum_{d=0}^{M-1} \left( \underbrace{\| \text{sg}[r_d] - \mathbf{e}_{c_d} \|^2}_{\text{Codebook Learning}} + \beta \underbrace{\| r_d - \text{sg}[\mathbf{e}_{c_d}] \|^2}_{\text{Commitment}} \right)$$
